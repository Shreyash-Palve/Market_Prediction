{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import janestreet\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import dump, load\n",
    "import datatable as dtable\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "TEST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted average as per Donate et al.'s formula\n",
    "# https://doi.org/10.1016/j.neucom.2012.02.053\n",
    "# [0.0625, 0.0625, 0.125, 0.25, 0.5] for 5 fold\n",
    "def weighted_average(a):\n",
    "    w = []\n",
    "    n = len(a)\n",
    "    for j in range(1, n + 1):\n",
    "        j = 2 if j == 1 else j\n",
    "        w.append(1 / (2**(n + 1 - j)))\n",
    "    return np.average(a, weights = w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    train = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv', nrows = 100)\n",
    "    features = [c for c in train.columns if 'feature' in c]\n",
    "else:\n",
    "    print('Loading...')\n",
    "    train = dtable.fread('../input/jane-street-market-prediction/train.csv').to_pandas()\n",
    "    features = [c for c in train.columns if 'feature' in c]\n",
    "\n",
    "    print('Filling...')\n",
    "    train = train.query('date > 85').reset_index(drop = True) \n",
    "    train = train.query('weight > 0').reset_index(drop = True)\n",
    "    train[features] = train[features].fillna(method = 'ffill').fillna(0)\n",
    "    train['action'] = ((train['resp_1'] > 0) & (train['resp_2'] > 0) & (train['resp_3'] > 0) & (train['resp_4'] > 0) & (train['resp'] > 0)).astype('int')\n",
    "\n",
    "    resp_cols = ['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4']\n",
    "\n",
    "    X = train[features].values\n",
    "    y = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T\n",
    "    date = train['date'].values\n",
    "    weight = train['weight'].values\n",
    "    resp = train['resp'].values\n",
    "    sw = np.mean(np.abs(train[resp_cols].values), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "group_gap = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ae_mlp(num_columns, num_labels, hidden_units, dropout_rates, ls = 1e-2, lr = 1e-3):\n",
    "    \n",
    "    inp = tf.keras.layers.Input(shape = (num_columns, ))\n",
    "    x0 = tf.keras.layers.BatchNormalization()(inp)\n",
    "    \n",
    "    encoder = tf.keras.layers.GaussianNoise(dropout_rates[0])(x0)\n",
    "    encoder = tf.keras.layers.Dense(hidden_units[0])(encoder)\n",
    "    encoder = tf.keras.layers.BatchNormalization()(encoder)\n",
    "    encoder = tf.keras.layers.Activation('swish')(encoder)\n",
    "    \n",
    "    decoder = tf.keras.layers.Dropout(dropout_rates[1])(encoder)\n",
    "    decoder = tf.keras.layers.Dense(num_columns, name = 'decoder')(decoder)\n",
    "\n",
    "    x_ae = tf.keras.layers.Dense(hidden_units[1])(decoder)\n",
    "    x_ae = tf.keras.layers.BatchNormalization()(x_ae)\n",
    "    x_ae = tf.keras.layers.Activation('swish')(x_ae)\n",
    "    x_ae = tf.keras.layers.Dropout(dropout_rates[2])(x_ae)\n",
    "\n",
    "    out_ae = tf.keras.layers.Dense(num_labels, activation = 'sigmoid', name = 'ae_action')(x_ae)\n",
    "    \n",
    "    x = tf.keras.layers.Concatenate()([x0, encoder])\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rates[3])(x)\n",
    "    \n",
    "    for i in range(2, len(hidden_units)):\n",
    "        x = tf.keras.layers.Dense(hidden_units[i])(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('swish')(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i + 2])(x)\n",
    "        \n",
    "    out = tf.keras.layers.Dense(num_labels, activation = 'sigmoid', name = 'action')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs = inp, outputs = [decoder, out_ae, out])\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = lr),\n",
    "                  loss = {'decoder': tf.keras.losses.MeanSquaredError(), \n",
    "                          'ae_action': tf.keras.losses.BinaryCrossentropy(label_smoothing = ls),\n",
    "                          'action': tf.keras.losses.BinaryCrossentropy(label_smoothing = ls), \n",
    "                         },\n",
    "                  metrics = {'decoder': tf.keras.metrics.MeanAbsoluteError(name = 'MAE'), \n",
    "                             'ae_action': tf.keras.metrics.AUC(name = 'AUC'), \n",
    "                             'action': tf.keras.metrics.AUC(name = 'AUC'), \n",
    "                            }, \n",
    "                 )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_columns': len(features), \n",
    "          'num_labels': 5, \n",
    "          'hidden_units': [96, 96, 896, 448, 448, 256], \n",
    "          'dropout_rates': [0.03527936123679956, 0.038424974585075086, 0.42409238408801436, 0.10431484318345882, 0.49230389137187497, 0.32024444956111164, 0.2716856145683449, 0.4379233941604448], \n",
    "          'ls': 0, \n",
    "          'lr':1e-3, \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TEST:\n",
    "    scores = []\n",
    "    batch_size = 4096\n",
    "    gkf = PurgedGroupTimeSeriesSplit(n_splits = n_splits, group_gap = group_gap)\n",
    "    for fold, (tr, te) in enumerate(gkf.split(train['action'].values, train['action'].values, train['date'].values)):\n",
    "        ckp_path = f'JSModel_{fold}.hdf5'\n",
    "        model = create_ae_mlp(**params)\n",
    "        ckp = ModelCheckpoint(ckp_path, monitor = 'val_action_AUC', verbose = 0, \n",
    "                              save_best_only = True, save_weights_only = True, mode = 'max')\n",
    "        es = EarlyStopping(monitor = 'val_action_AUC', min_delta = 1e-4, patience = 10, mode = 'max', \n",
    "                           baseline = None, restore_best_weights = True, verbose = 0)\n",
    "        history = model.fit(X[tr], [X[tr], y[tr], y[tr]], validation_data = (X[te], [X[te], y[te], y[te]]), \n",
    "                            sample_weight = sw[tr], \n",
    "                            epochs = 100, batch_size = batch_size, callbacks = [ckp, es], verbose = 0)\n",
    "        hist = pd.DataFrame(history.history)\n",
    "        score = hist['val_action_AUC'].max()\n",
    "        print(f'Fold {fold} ROC AUC:\\t', score)\n",
    "        scores.append(score)\n",
    "\n",
    "        K.clear_session()\n",
    "        del model\n",
    "        rubbish = gc.collect()\n",
    "    \n",
    "    print('Weighted Average CV Score:', weighted_average(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
